{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aocd\n",
    "from aocd.models import Puzzle\n",
    "day = 7\n",
    "year = 2021\n",
    "puzzle = Puzzle(year=year, day=day)\n",
    "# data = aocd.get_data(day=day, year=year)\n",
    "with open('./data/input_{:02d}'.format(day), 'w') as fh:\n",
    "    fh.write(puzzle.input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, brute, bisect, minimize_scalar, bracket\n",
    "from scipy.special import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " ['1101,1,29,67,1102,0,1,65,1008,65,35,66,1005,66,28,1,67,65,20,4,0,1001,65,1,65,1106,0,8,99,35,67,101,99,105,32,110,39,101,115,116,32,112,97,115,32,117,110,101,32,105,110,116,99,111,100,101,32,112,114,111,103,114,97,109,10,76,50,481,307,723,1100,235,147,851,504,1068,780,490,126,771,831,358,711,68,281,645,644,862,705,372,217,679,1097,1005,728,1739,571,40,1293,782,378,802,245,1370,1512,9,372,448,64,36,1212,141,585,1468,278,286,476,239,285,664,277,43,665,1037,654,205,1238,36,41,276,945,624,47,91,1569,284,107,845,60,961,30,21,269,1091,304,364,339,169,430,176,156,1483,1510,111,146,447,590,1227,611,483,428,396,839,307,901,380,128,80,535,461,482,379,59,281,977,44,966,545,37,163,845,845,151,936,269,938,612,1284,863,516,927,511,825,125,599,101,394,1062,140,483,218,83,443,404,492,78,507,860,1741,43,128,60,566,181,5,554,937,163,280,655,184,480,408,127,935,613,766,46,312,770,697,275,13,84,600,168,83,281,599,607,1441,197,344,0,302,414,147,370,748,421,844,871,319,666,117,640,247,167,529,324,252,235,303,443,1305,796,109,20,364,532,1388,708,769,916,340,405,90,47,504,516,97,535,28,69,960,590,254,106,188,190,1388,698,246,264,98,229,1648,292,710,14,421,31,147,1493,552,1371,454,4,146,674,452,1267,1027,170,141,936,1341,884,558,276,631,68,39,2,464,1,839,318,881,413,2,452,352,34,89,323,884,1439,243,79,56,128,1273,1134,606,11,682,747,415,599,782,179,269,320,682,177,336,466,10,370,159,1636,367,888,573,171,682,60,9,59,332,10,1496,637,1029,413,186,1183,77,309,461,883,1079,699,233,69,259,108,1160,435,480,495,13,858,718,126,115,728,1008,133,442,7,598,1475,1156,226,162,415,3,151,72,527,792,494,763,144,64,490,273,1245,300,465,744,36,1465,251,8,494,1126,362,180,1263,175,141,1041,103,163,205,568,93,699,103,437,204,931,563,550,88,415,146,265,31,221,1123,835,375,1101,578,388,92,1417,845,308,343,499,158,293,242,4,509,574,254,1556,69,668,691,0,558,16,687,1210,166,748,400,863,66,600,771,1073,561,738,398,384,232,350,393,1113,1222,153,462,907,797,712,18,1463,1185,1055,994,57,130,265,131,52,463,902,453,38,132,783,1560,232,169,1162,173,311,5,1477,397,336,480,540,491,67,340,27,291,341,35,275,78,1525,387,218,63,79,533,4,569,1643,595,1508,851,39,1200,912,10,53,42,60,154,1174,155,275,137,677,367,1373,4,708,441,756,647,1054,872,1039,109,530,1179,939,429,567,866,1411,436,23,212,184,66,79,831,538,90,827,678,549,313,434,60,907,284,171,570,1091,603,447,122,1092,29,789,563,462,15,310,340,16,365,393,614,48,368,42,457,736,737,1008,513,61,764,366,400,525,1683,1177,909,908,112,734,16,79,917,541,127,107,79,1208,32,258,596,166,376,1313,735,1457,864,563,55,439,54,694,81,93,48,470,1028,689,1177,1331,155,412,847,250,405,387,8,456,18,619,533,729,1475,1182,935,210,55,355,958,15,32,598,85,175,471,1087,280,652,53,13,225,12,488,717,353,2,134,351,698,276,456,209,535,604,19,12,785,3,63,879,437,216,1,1275,811,786,417,33,51,733,1074,143,309,65,555,557,78,611,909,260,973,701,998,490,213,9,233,760,933,916,437,1369,1952,372,324,859,670,73,296,1391,127,407,230,52,16,547,803,883,258,308,710,343,1290,184,8,41,9,68,104,175,1034,1544,219,752,327,690,134,601,1574,214,385,1233,231,267,944,1533,349,431,97,632,278,1505,162,888,62,90,489,351,990,846,14,159,134,14,314,148,214,1153,513,114,6,49,10,14,957,219,16,204,954,863,50,482,90,696,99,253,252,433,57,385,54,343,106,154,78,1595,590,380,102,825,1933,191,1328,374,263,355,137,494,60,781,1113,391,274,325,326,14,965,269,15,32,742,81,393,730,892,982,103,890,499,58,816,292,29,480,173,831,132,1033,1511,1137,1511,22,1105,146,344,308,915,540,1371,1238,414,352,304,841,749,6,491,30,1322,415,293,1207,31,90,636,303,1551,354,23,275,18,32,623,1483,49,12,311,407,1551,296,252,647,778,1499,98,1220,264,1020,1440,377,1125,8,72,270,162,348,3,1023,965,719,62,1467,1176,663,439,557,654,85,1493,70,349,10,727,15,1173,387,529,608,1398,905,619,173,849,1493,49,88,4,708,1084,370,1007,285,4,530,770,561,26,669,1100,30,876,649,178,32,354,621,911,334,514,9,449,1019,107'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = puzzle.input_data.splitlines()\n",
    "len(data), data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = list(map(int, data[0].split(',')))\n",
    "# pos = list(map(int, \"\"\"16,1,2,0,4,2,7,1,2,14\"\"\".split(',')))\n",
    "pos = np.array(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle.answer_a = int(np.abs(pos - np.median(pos)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_consumption = binom(np.arange(1, pos.max()+1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   3.,   6.,  10.,  15.,  21.,  28.,  36.,  45.,  55.,\n",
       "        66.,  78.,  91., 105., 120.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuel_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuel_consumption[np.abs(pos-5).astype(np.int32)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuel(target):\n",
    "    print(target)    \n",
    "    dist = np.abs(pos-target).astype(np.int32)\n",
    "    try:\n",
    "        return fuel_consumption[dist].sum()\n",
    "    except IndexError:\n",
    "        return np.inf\n",
    "\n",
    "# minimize(fuel, np.median(pos), method=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, idx, vals = brute(fuel, (slice(pos.min()+1,pos.max()-1,1),), full_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[vals.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThat's the right answer!  You are one gold star closer to finding the sleigh keys.You have completed Day 7! You can [Shareon\n",
      "  Twitter\n",
      "Mastodon] this victory or [Return to Your Advent Calendar].\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "puzzle.answer_b = int(vals.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "26.18034\n",
      "52.36068025156\n",
      "232.61449024813282\n",
      "524.2712834521276\n",
      "480.86836154867365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(232.61449024813282,\n",
       " 480.86836154867365,\n",
       " 524.2712834521276,\n",
       " 120242172.0,\n",
       " 89473804.0,\n",
       " 90438900.0,\n",
       " 7)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bracket(fuel, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745.5976539602052\n",
      "1206.4023460397946\n",
      "460.8046920795895\n",
      "480.7839522162522\n",
      "480.8561349793511\n",
      "480.82004359780166\n",
      "480.8062579167507\n",
      "480.79773789730314\n",
      "480.79247223569973\n",
      "480.7892178778556\n",
      "480.7872065740964\n",
      "480.7859635200115\n",
      "480.7851952703371\n",
      "480.7847204659265\n",
      "480.78442702066275\n",
      "480.7842456615159\n",
      "480.78413357539904\n",
      "480.78406430236913\n",
      "480.7840214892821\n",
      "480.78399502933917\n",
      "480.7839786761951\n",
      "480.7839682116829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: 89473804.0\n",
       " message: 'Solution found.'\n",
       "    nfev: 22\n",
       "  status: 0\n",
       " success: True\n",
       "       x: 480.7839682116829"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_scalar(fuel, bounds=(0, 1952), method='bounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "========\n",
      "minimize\n",
      "========\n",
      "\n",
      "\n",
      "bfgs\n",
      "====\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "BFGS algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "cg\n",
      "==\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "conjugate gradient algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "norm : float\n",
      "    Order of norm (Inf is max, -Inf is min).\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "cobyla\n",
      "======\n",
      "\n",
      "Minimize a scalar function of one or more variables using the\n",
      "Constrained Optimization BY Linear Approximation (COBYLA) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "rhobeg : float\n",
      "    Reasonable initial changes to the variables.\n",
      "tol : float\n",
      "    Final accuracy in the optimization (not precisely guaranteed).\n",
      "    This is a lower bound on the size of the trust region.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored as set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of function evaluations.\n",
      "catol : float\n",
      "    Tolerance (absolute) for constraint violations\n",
      "\n",
      "dogleg\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the dog-leg trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "l-bfgs-b\n",
      "========\n",
      "\n",
      "Minimize a scalar function of one or more variables using the L-BFGS-B\n",
      "algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : None or int\n",
      "    If `disp is None` (the default), then the supplied version of `iprint`\n",
      "    is used. If `disp is not None`, then it overrides the supplied version\n",
      "    of `iprint` with the behaviour you outlined.\n",
      "maxcor : int\n",
      "    The maximum number of variable metric corrections used to\n",
      "    define the limited memory matrix. (The limited memory BFGS\n",
      "    method does not store the full hessian but uses this many terms\n",
      "    in an approximation to it.)\n",
      "ftol : float\n",
      "    The iteration stops when ``(f^k -\n",
      "    f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n",
      "gtol : float\n",
      "    The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n",
      "    <= gtol`` where ``pg_i`` is the i-th component of the\n",
      "    projected gradient.\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "maxfun : int\n",
      "    Maximum number of function evaluations.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "iprint : int, optional\n",
      "    Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "    ``iprint = 0``    print only one line at the last iteration;\n",
      "    ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "    ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "    ``iprint = 100``  print also the changes of active set and final x;\n",
      "    ``iprint > 100``  print details of every iteration including x and g.\n",
      "callback : callable, optional\n",
      "    Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "    current parameter vector.\n",
      "maxls : int, optional\n",
      "    Maximum number of line search steps (per iteration). Default is 20.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The option `ftol` is exposed via the `scipy.optimize.minimize` interface,\n",
      "but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The\n",
      "relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.\n",
      "I.e., `factr` multiplies the default machine floating-point precision to\n",
      "arrive at `ftol`.\n",
      "\n",
      "nelder-mead\n",
      "===========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Nelder-Mead algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*200``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "initial_simplex : array_like of shape (N + 1, N)\n",
      "    Initial simplex. If given, overrides `x0`.\n",
      "    ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "    the jth vertex of the ``N+1`` vertices in the simplex, where\n",
      "    ``N`` is the dimension.\n",
      "xatol : float, optional\n",
      "    Absolute error in xopt between iterations that is acceptable for\n",
      "    convergence.\n",
      "fatol : number, optional\n",
      "    Absolute error in func(xopt) between iterations that is acceptable for\n",
      "    convergence.\n",
      "adaptive : bool, optional\n",
      "    Adapt algorithm parameters to dimensionality of problem. Useful for\n",
      "    high-dimensional minimization [1]_.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Gao, F. and Han, L.\n",
      "   Implementing the Nelder-Mead simplex algorithm with adaptive\n",
      "   parameters. 2012. Computational Optimization and Applications.\n",
      "   51:1, pp. 259-277\n",
      "\n",
      "newton-cg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "Newton-CG algorithm.\n",
      "\n",
      "Note that the `jac` parameter (Jacobian) is required.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Average relative error in solution `xopt` acceptable for\n",
      "    convergence.\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "eps : float or ndarray\n",
      "    If `hessp` is approximated, use this value for the step size.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "\n",
      "powell\n",
      "======\n",
      "\n",
      "Minimization of scalar function of one or more variables using the\n",
      "modified Powell algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "disp : bool\n",
      "    Set to True to print convergence messages.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "ftol : float\n",
      "    Relative error in ``fun(xopt)`` acceptable for convergence.\n",
      "maxiter, maxfev : int\n",
      "    Maximum allowed number of iterations and function evaluations.\n",
      "    Will default to ``N*1000``, where ``N`` is the number of\n",
      "    variables, if neither `maxiter` or `maxfev` is set. If both\n",
      "    `maxiter` and `maxfev` are set, minimization will stop at the\n",
      "    first reached.\n",
      "direc : ndarray\n",
      "    Initial set of direction vectors for the Powell method.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "bounds : `Bounds`\n",
      "    If bounds are not provided, then an unbounded line search will be used.\n",
      "    If bounds are provided and the initial guess is within the bounds, then\n",
      "    every function evaluation throughout the minimization procedure will be\n",
      "    within the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (or left to default), then some\n",
      "    function evaluations during the first iteration may be outside the\n",
      "    bounds, but every function evaluation after the first iteration will be\n",
      "    within the bounds. If `direc` is not full rank, then some parameters may\n",
      "    not be optimized and the solution is not guaranteed to be within the\n",
      "    bounds.\n",
      "return_all : bool, optional\n",
      "    Set to True to return a list of the best solution at each of the\n",
      "    iterations.\n",
      "\n",
      "slsqp\n",
      "=====\n",
      "\n",
      "Minimize a scalar function of one or more variables using Sequential\n",
      "Least Squares Programming (SLSQP).\n",
      "\n",
      "Options\n",
      "-------\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "eps : float\n",
      "    Step size used for numerical approximation of the Jacobian.\n",
      "disp : bool\n",
      "    Set to True to print convergence messages. If False,\n",
      "    `verbosity` is ignored and set to 0.\n",
      "maxiter : int\n",
      "    Maximum number of iterations.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of `jac`. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "\n",
      "tnc\n",
      "===\n",
      "\n",
      "Minimize a scalar function of one or more variables using a truncated\n",
      "Newton (TNC) algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "eps : float or ndarray\n",
      "    If `jac is None` the absolute step size used for numerical\n",
      "    approximation of the jacobian via forward differences.\n",
      "scale : list of floats\n",
      "    Scaling factors to apply to each variable. If None, the\n",
      "    factors are up-low for interval bounded variables and\n",
      "    1+|x] fo the others. Defaults to None.\n",
      "offset : float\n",
      "    Value to subtract from each variable. If None, the\n",
      "    offsets are (up+low)/2 for interval bounded variables\n",
      "    and x for the others.\n",
      "disp : bool\n",
      "   Set to True to print convergence messages.\n",
      "maxCGit : int\n",
      "    Maximum number of hessian*vector evaluations per main\n",
      "    iteration. If maxCGit == 0, the direction chosen is\n",
      "    -gradient if maxCGit < 0, maxCGit is set to\n",
      "    max(1,min(50,n/2)). Defaults to -1.\n",
      "maxiter : int, optional\n",
      "    Maximum number of function evaluations. This keyword is deprecated\n",
      "    in favor of `maxfun`. Only if `maxfun` is None is this keyword used.\n",
      "eta : float\n",
      "    Severity of the line search. If < 0 or > 1, set to 0.25.\n",
      "    Defaults to -1.\n",
      "stepmx : float\n",
      "    Maximum step for the line search. May be increased during\n",
      "    call. If too small, it will be set to 10.0. Defaults to 0.\n",
      "accuracy : float\n",
      "    Relative precision for finite difference calculations. If\n",
      "    <= machine_precision, set to sqrt(machine_precision).\n",
      "    Defaults to 0.\n",
      "minfev : float\n",
      "    Minimum function value estimate. Defaults to 0.\n",
      "ftol : float\n",
      "    Precision goal for the value of f in the stopping criterion.\n",
      "    If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "xtol : float\n",
      "    Precision goal for the value of x in the stopping\n",
      "    criterion (after applying x scaling factors). If xtol <\n",
      "    0.0, xtol is set to sqrt(machine_precision). Defaults to\n",
      "    -1.\n",
      "gtol : float\n",
      "    Precision goal for the value of the projected gradient in\n",
      "    the stopping criterion (after applying x scaling factors).\n",
      "    If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
      "    Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "rescale : float\n",
      "    Scaling factor (in log10) used to trigger f value\n",
      "    rescaling.  If 0, rescale at each iteration.  If a large\n",
      "    value, never rescale.  If < 0, rescale is set to 1.3.\n",
      "finite_diff_rel_step : None or array_like, optional\n",
      "    If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n",
      "    use for numerical approximation of the jacobian. The absolute step\n",
      "    size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n",
      "    possibly adjusted to fit into the bounds. For ``method='3-point'``\n",
      "    the sign of `h` is ignored. If None (default) then step is selected\n",
      "    automatically.\n",
      "maxfun : int\n",
      "    Maximum number of function evaluations. If None, `maxfun` is\n",
      "    set to max(100, 10*len(x0)). Defaults to None.\n",
      "\n",
      "trust-ncg\n",
      "=========\n",
      "\n",
      "Minimization of scalar function of one or more variables using\n",
      "the Newton conjugate gradient trust-region algorithm.\n",
      "\n",
      "Options\n",
      "-------\n",
      "initial_trust_radius : float\n",
      "    Initial trust-region radius.\n",
      "max_trust_radius : float\n",
      "    Maximum value of the trust-region radius. No steps that are longer\n",
      "    than this value will be proposed.\n",
      "eta : float\n",
      "    Trust region related acceptance stringency for proposed steps.\n",
      "gtol : float\n",
      "    Gradient norm must be less than `gtol` before successful\n",
      "    termination.\n",
      "\n",
      "===============\n",
      "minimize_scalar\n",
      "===============\n",
      "\n",
      "\n",
      "brent\n",
      "=====\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Uses inverse parabolic interpolation when possible to speed up\n",
      "convergence of golden section method.\n",
      "\n",
      "bounded\n",
      "=======\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "disp: int, optional\n",
      "    If non-zero, print messages.\n",
      "        0 : no message printing.\n",
      "        1 : non-convergence notification messages only.\n",
      "        2 : print a message on convergence too.\n",
      "        3 : print iteration results.\n",
      "xatol : float\n",
      "    Absolute error in solution `xopt` acceptable for convergence.\n",
      "\n",
      "golden\n",
      "======\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "    Maximum number of iterations to perform.\n",
      "xtol : float\n",
      "    Relative error in solution `xopt` acceptable for convergence.\n",
      "\n",
      "\n",
      "====\n",
      "root\n",
      "====\n",
      "\n",
      "\n",
      "hybr\n",
      "====\n",
      "\n",
      "Find the roots of a multivariate function using MINPACK's hybrd and\n",
      "hybrj routines (modified Powell method).\n",
      "\n",
      "Options\n",
      "-------\n",
      "col_deriv : bool\n",
      "    Specify whether the Jacobian function computes derivatives down\n",
      "    the columns (faster, because there is no transpose operation).\n",
      "xtol : float\n",
      "    The calculation will terminate if the relative error between two\n",
      "    consecutive iterates is at most `xtol`.\n",
      "maxfev : int\n",
      "    The maximum number of calls to the function. If zero, then\n",
      "    ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "    in `x0`.\n",
      "band : tuple\n",
      "    If set to a two-sequence containing the number of sub- and\n",
      "    super-diagonals within the band of the Jacobi matrix, the\n",
      "    Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "eps : float\n",
      "    A suitable step length for the forward-difference\n",
      "    approximation of the Jacobian (for ``fprime=None``). If\n",
      "    `eps` is less than the machine precision, it is assumed\n",
      "    that the relative errors in the functions are of the order of\n",
      "    the machine precision.\n",
      "factor : float\n",
      "    A parameter determining the initial step bound\n",
      "    (``factor * || diag * x||``). Should be in the interval\n",
      "    ``(0.1, 100)``.\n",
      "diag : sequence\n",
      "    N positive entries that serve as a scale factors for the\n",
      "    variables.\n",
      "\n",
      "lm\n",
      "==\n",
      "\n",
      "Solve for least squares with Levenberg-Marquardt\n",
      "\n",
      "Options\n",
      "-------\n",
      "col_deriv : bool\n",
      "    non-zero to specify that the Jacobian function computes derivatives\n",
      "    down the columns (faster, because there is no transpose operation).\n",
      "ftol : float\n",
      "    Relative error desired in the sum of squares.\n",
      "xtol : float\n",
      "    Relative error desired in the approximate solution.\n",
      "gtol : float\n",
      "    Orthogonality desired between the function vector and the columns\n",
      "    of the Jacobian.\n",
      "maxiter : int\n",
      "    The maximum number of calls to the function. If zero, then\n",
      "    100*(N+1) is the maximum where N is the number of elements in x0.\n",
      "epsfcn : float\n",
      "    A suitable step length for the forward-difference approximation of\n",
      "    the Jacobian (for Dfun=None). If epsfcn is less than the machine\n",
      "    precision, it is assumed that the relative errors in the functions\n",
      "    are of the order of the machine precision.\n",
      "factor : float\n",
      "    A parameter determining the initial step bound\n",
      "    (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "diag : sequence\n",
      "    N positive entries that serve as a scale factors for the variables.\n",
      "\n",
      "broyden1\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden\n",
      "            matrix stays low. Can either be a string giving the\n",
      "            name of the method, or a tuple of the form ``(method,\n",
      "            param1, param2, ...)`` that gives the name of the\n",
      "            method and values for additional parameters.\n",
      "\n",
      "            Methods available:\n",
      "\n",
      "                - ``restart``\n",
      "                    Drop all matrix columns. Has no\n",
      "                    extra parameters.\n",
      "                - ``simple``\n",
      "                    Drop oldest matrix column. Has no\n",
      "                    extra parameters.\n",
      "                - ``svd``\n",
      "                    Keep only the most significant SVD\n",
      "                    components.\n",
      "\n",
      "                    Extra parameters:\n",
      "\n",
      "                        - ``to_retain``\n",
      "                            Number of SVD components to\n",
      "                            retain when rank reduction is done.\n",
      "                            Default is ``max_rank - 2``.\n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "\n",
      "broyden2\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial guess for the Jacobian is (-1/alpha).\n",
      "    reduction_method : str or tuple, optional\n",
      "        Method used in ensuring that the rank of the Broyden\n",
      "        matrix stays low. Can either be a string giving the\n",
      "        name of the method, or a tuple of the form ``(method,\n",
      "        param1, param2, ...)`` that gives the name of the\n",
      "        method and values for additional parameters.\n",
      "\n",
      "        Methods available:\n",
      "\n",
      "            - ``restart``\n",
      "                Drop all matrix columns. Has no\n",
      "                extra parameters.\n",
      "            - ``simple``\n",
      "                Drop oldest matrix column. Has no\n",
      "                extra parameters.\n",
      "            - ``svd``\n",
      "                Keep only the most significant SVD\n",
      "                components.\n",
      "\n",
      "                Extra parameters:\n",
      "\n",
      "                    - ``to_retain``\n",
      "                        Number of SVD components to\n",
      "                        retain when rank reduction is done.\n",
      "                        Default is ``max_rank - 2``.\n",
      "    max_rank : int, optional\n",
      "        Maximum rank for the Broyden matrix.\n",
      "        Default is infinity (i.e., no rank reduction).\n",
      "\n",
      "anderson\n",
      "========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial guess for the Jacobian is (-1/alpha).\n",
      "    M : float, optional\n",
      "        Number of previous vectors to retain. Defaults to 5.\n",
      "    w0 : float, optional\n",
      "        Regularization parameter for numerical stability.\n",
      "        Compared to unity, good values of the order of 0.01.\n",
      "\n",
      "diagbroyden\n",
      "===========\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        initial guess for the jacobian is (-1/alpha).\n",
      "\n",
      "excitingmixing\n",
      "==============\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        Initial Jacobian approximation is (-1/alpha).\n",
      "    alphamax : float, optional\n",
      "        The entries of the diagonal Jacobian are kept in the range\n",
      "        ``[alpha, alphamax]``.\n",
      "\n",
      "linearmixing\n",
      "============\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, ``NoConvergence`` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    alpha : float, optional\n",
      "        initial guess for the jacobian is (-1/alpha).\n",
      "\n",
      "krylov\n",
      "======\n",
      "\n",
      "Options\n",
      "-------\n",
      "nit : int, optional\n",
      "    Number of iterations to make. If omitted (default), make as many\n",
      "    as required to meet tolerances.\n",
      "disp : bool, optional\n",
      "    Print status to stdout on every iteration.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations to make. If more are needed to\n",
      "    meet convergence, `NoConvergence` is raised.\n",
      "ftol : float, optional\n",
      "    Relative tolerance for the residual. If omitted, not used.\n",
      "fatol : float, optional\n",
      "    Absolute tolerance (in max-norm) for the residual.\n",
      "    If omitted, default is 6e-6.\n",
      "xtol : float, optional\n",
      "    Relative minimum step size. If omitted, not used.\n",
      "xatol : float, optional\n",
      "    Absolute minimum step size, as determined from the Jacobian\n",
      "    approximation. If the step size is smaller than this, optimization\n",
      "    is terminated as successful. If omitted, not used.\n",
      "tol_norm : function(vector) -> scalar, optional\n",
      "    Norm to use in convergence check. Default is the maximum norm.\n",
      "line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "    Which type of a line search to use to determine the step size in\n",
      "    the direction given by the Jacobian approximation. Defaults to\n",
      "    'armijo'.\n",
      "jac_options : dict, optional\n",
      "    Options for the respective Jacobian approximation.\n",
      "\n",
      "    rdiff : float, optional\n",
      "        Relative step size to use in numerical differentiation.\n",
      "    method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
      "        Krylov method to use to approximate the Jacobian.\n",
      "        Can be a string, or a function implementing the same\n",
      "        interface as the iterative solvers in\n",
      "        `scipy.sparse.linalg`.\n",
      "\n",
      "        The default is `scipy.sparse.linalg.lgmres`.\n",
      "    inner_M : LinearOperator or InverseJacobian\n",
      "        Preconditioner for the inner Krylov iteration.\n",
      "        Note that you can use also inverse Jacobians as (adaptive)\n",
      "        preconditioners. For example,\n",
      "\n",
      "        >>> jac = BroydenFirst()\n",
      "        >>> kjac = KrylovJacobian(inner_M=jac.inverse).\n",
      "\n",
      "        If the preconditioner has a method named 'update', it will\n",
      "        be called as ``update(x, f)`` after each nonlinear step,\n",
      "        with ``x`` giving the current point, and ``f`` the current\n",
      "        function value.\n",
      "    inner_tol, inner_maxiter, ...\n",
      "        Parameters to pass on to the \"inner\" Krylov solver.\n",
      "        See `scipy.sparse.linalg.gmres` for details.\n",
      "    outer_k : int, optional\n",
      "        Size of the subspace kept across LGMRES nonlinear\n",
      "        iterations.\n",
      "\n",
      "        See `scipy.sparse.linalg.lgmres` for details.\n",
      "\n",
      "df-sane\n",
      "=======\n",
      "\n",
      "Solve nonlinear equation with the DF-SANE method\n",
      "\n",
      "Options\n",
      "-------\n",
      "ftol : float, optional\n",
      "    Relative norm tolerance.\n",
      "fatol : float, optional\n",
      "    Absolute norm tolerance.\n",
      "    Algorithm terminates when ``||func(x)|| < fatol + ftol ||func(x_0)||``.\n",
      "fnorm : callable, optional\n",
      "    Norm to use in the convergence check. If None, 2-norm is used.\n",
      "maxfev : int, optional\n",
      "    Maximum number of function evaluations.\n",
      "disp : bool, optional\n",
      "    Whether to print convergence process to stdout.\n",
      "eta_strategy : callable, optional\n",
      "    Choice of the ``eta_k`` parameter, which gives slack for growth\n",
      "    of ``||F||**2``.  Called as ``eta_k = eta_strategy(k, x, F)`` with\n",
      "    `k` the iteration number, `x` the current iterate and `F` the current\n",
      "    residual. Should satisfy ``eta_k > 0`` and ``sum(eta, k=0..inf) < inf``.\n",
      "    Default: ``||F||**2 / (1 + k)**2``.\n",
      "sigma_eps : float, optional\n",
      "    The spectral coefficient is constrained to ``sigma_eps < sigma < 1/sigma_eps``.\n",
      "    Default: 1e-10\n",
      "sigma_0 : float, optional\n",
      "    Initial spectral coefficient.\n",
      "    Default: 1.0\n",
      "M : int, optional\n",
      "    Number of iterates to include in the nonmonotonic line search.\n",
      "    Default: 10\n",
      "line_search : {'cruz', 'cheng'}\n",
      "    Type of line search to employ. 'cruz' is the original one defined in\n",
      "    [Martinez & Raydan. Math. Comp. 75, 1429 (2006)], 'cheng' is\n",
      "    a modified search defined in [Cheng & Li. IMA J. Numer. Anal. 29, 814 (2009)].\n",
      "    Default: 'cruz'\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] \"Spectral residual method without gradient information for solving\n",
      "       large-scale nonlinear systems of equations.\" W. La Cruz,\n",
      "       J.M. Martinez, M. Raydan. Math. Comp. **75**, 1429 (2006).\n",
      ".. [2] W. La Cruz, Opt. Meth. Software, 29, 24 (2014).\n",
      ".. [3] W. Cheng, D.-H. Li. IMA J. Numer. Anal. **29**, 814 (2009).\n",
      "\n",
      "\n",
      "=======\n",
      "linprog\n",
      "=======\n",
      "\n",
      "\n",
      "simplex\n",
      "=======\n",
      "\n",
      "Minimize a linear objective function subject to linear equality and\n",
      "non-negativity constraints using the two phase simplex method.\n",
      "Linear programming is intended to solve problems of the following form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A @ x == b\n",
      "        x >= 0\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    Coefficients of the linear objective function to be minimized.\n",
      "c0 : float\n",
      "    Constant term in objective function due to fixed (and eliminated)\n",
      "    variables. (Purely for display.)\n",
      "A : 2-D array\n",
      "    2-D array such that ``A @ x``, gives the values of the equality\n",
      "    constraints at ``x``.\n",
      "b : 1-D array\n",
      "    1-D array of values representing the right hand side of each equality\n",
      "    constraint (row) in ``A``.\n",
      "callback : callable, optional\n",
      "    If a callback function is provided, it will be called within each\n",
      "    iteration of the algorithm. The callback function must accept a single\n",
      "    `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "\n",
      "        x : 1-D array\n",
      "            Current solution vector\n",
      "        fun : float\n",
      "            Current value of the objective function\n",
      "        success : bool\n",
      "            True when an algorithm has completed successfully.\n",
      "        slack : 1-D array\n",
      "            The values of the slack variables. Each slack variable\n",
      "            corresponds to an inequality constraint. If the slack is zero,\n",
      "            the corresponding constraint is active.\n",
      "        con : 1-D array\n",
      "            The (nominally zero) residuals of the equality constraints,\n",
      "            that is, ``b - A_eq @ x``\n",
      "        phase : int\n",
      "            The phase of the algorithm being executed.\n",
      "        status : int\n",
      "            An integer representing the status of the optimization::\n",
      "\n",
      "                 0 : Algorithm proceeding nominally\n",
      "                 1 : Iteration limit reached\n",
      "                 2 : Problem appears to be infeasible\n",
      "                 3 : Problem appears to be unbounded\n",
      "                 4 : Serious numerical difficulties encountered\n",
      "        nit : int\n",
      "            The number of iterations performed.\n",
      "        message : str\n",
      "            A string descriptor of the exit status of the optimization.\n",
      "postsolve_args : tuple\n",
      "    Data needed by _postsolve to convert the solution to the standard-form\n",
      "    problem into the solution to the original problem.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int\n",
      "   The maximum number of iterations to perform.\n",
      "disp : bool\n",
      "    If True, print exit status message to sys.stdout\n",
      "tol : float\n",
      "    The tolerance which determines when a solution is \"close enough\" to\n",
      "    zero in Phase 1 to be considered a basic feasible solution or close\n",
      "    enough to positive to serve as an optimal solution.\n",
      "bland : bool\n",
      "    If True, use Bland's anti-cycling rule [3]_ to choose pivots to\n",
      "    prevent cycling. If False, choose pivots which should lead to a\n",
      "    converged solution more quickly. The latter method is subject to\n",
      "    cycling (non-convergence) in rare instances.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    `unknown_options` is non-empty a warning is issued listing all\n",
      "    unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "x : 1-D array\n",
      "    Solution vector.\n",
      "status : int\n",
      "    An integer representing the exit status of the optimization::\n",
      "\n",
      "     0 : Optimization terminated successfully\n",
      "     1 : Iteration limit reached\n",
      "     2 : Problem appears to be infeasible\n",
      "     3 : Problem appears to be unbounded\n",
      "     4 : Serious numerical difficulties encountered\n",
      "\n",
      "message : str\n",
      "    A string descriptor of the exit status of the optimization.\n",
      "iteration : int\n",
      "    The number of iterations taken to solve the problem.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "       Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "       1963\n",
      ".. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "       Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      ".. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "       Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The expected problem formulation differs between the top level ``linprog``\n",
      "module and the method specific solvers. The method specific solvers expect a\n",
      "problem in standard form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A @ x == b\n",
      "        x >= 0\n",
      "\n",
      "Whereas the top level ``linprog`` module expects a problem of form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "     lb <= x <= ub\n",
      "\n",
      "where ``lb = 0`` and ``ub = None`` unless set in ``bounds``.\n",
      "\n",
      "The original problem contains equality, upper-bound and variable constraints\n",
      "whereas the method specific solver requires equality constraints and\n",
      "variable non-negativity.\n",
      "\n",
      "``linprog`` module converts the original problem to standard form by\n",
      "converting the simple bounds to upper bound constraints, introducing\n",
      "non-negative slack variables for inequality constraints, and expressing\n",
      "unbounded variables as the difference between two non-negative variables.\n",
      "\n",
      "interior-point\n",
      "==============\n",
      "\n",
      "Minimize a linear objective function subject to linear\n",
      "equality and non-negativity constraints using the interior point method\n",
      "of [4]_. Linear programming is intended to solve problems\n",
      "of the following form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A @ x == b\n",
      "        x >= 0\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "c : 1-D array\n",
      "    Coefficients of the linear objective function to be minimized.\n",
      "c0 : float\n",
      "    Constant term in objective function due to fixed (and eliminated)\n",
      "    variables. (Purely for display.)\n",
      "A : 2-D array\n",
      "    2-D array such that ``A @ x``, gives the values of the equality\n",
      "    constraints at ``x``.\n",
      "b : 1-D array\n",
      "    1-D array of values representing the right hand side of each equality\n",
      "    constraint (row) in ``A``.\n",
      "callback : callable, optional\n",
      "    Callback function to be executed once per iteration.\n",
      "postsolve_args : tuple\n",
      "    Data needed by _postsolve to convert the solution to the standard-form\n",
      "    problem into the solution to the original problem.\n",
      "\n",
      "Options\n",
      "-------\n",
      "maxiter : int (default = 1000)\n",
      "    The maximum number of iterations of the algorithm.\n",
      "tol : float (default = 1e-8)\n",
      "    Termination tolerance to be used for all termination criteria;\n",
      "    see [4]_ Section 4.5.\n",
      "disp : bool (default = False)\n",
      "    Set to ``True`` if indicators of optimization status are to be printed\n",
      "    to the console each iteration.\n",
      "alpha0 : float (default = 0.99995)\n",
      "    The maximal step size for Mehrota's predictor-corrector search\n",
      "    direction; see :math:`\\beta_{3}` of [4]_ Table 8.1.\n",
      "beta : float (default = 0.1)\n",
      "    The desired reduction of the path parameter :math:`\\mu` (see [6]_)\n",
      "    when Mehrota's predictor-corrector is not in use (uncommon).\n",
      "sparse : bool (default = False)\n",
      "    Set to ``True`` if the problem is to be treated as sparse after\n",
      "    presolve. If either ``A_eq`` or ``A_ub`` is a sparse matrix,\n",
      "    this option will automatically be set ``True``, and the problem\n",
      "    will be treated as sparse even during presolve. If your constraint\n",
      "    matrices contain mostly zeros and the problem is not very small (less\n",
      "    than about 100 constraints or variables), consider setting ``True``\n",
      "    or providing ``A_eq`` and ``A_ub`` as sparse matrices.\n",
      "lstsq : bool (default = False)\n",
      "    Set to ``True`` if the problem is expected to be very poorly\n",
      "    conditioned. This should always be left ``False`` unless severe\n",
      "    numerical difficulties are encountered. Leave this at the default\n",
      "    unless you receive a warning message suggesting otherwise.\n",
      "sym_pos : bool (default = True)\n",
      "    Leave ``True`` if the problem is expected to yield a well conditioned\n",
      "    symmetric positive definite normal equation matrix\n",
      "    (almost always). Leave this at the default unless you receive\n",
      "    a warning message suggesting otherwise.\n",
      "cholesky : bool (default = True)\n",
      "    Set to ``True`` if the normal equations are to be solved by explicit\n",
      "    Cholesky decomposition followed by explicit forward/backward\n",
      "    substitution. This is typically faster for problems\n",
      "    that are numerically well-behaved.\n",
      "pc : bool (default = True)\n",
      "    Leave ``True`` if the predictor-corrector method of Mehrota is to be\n",
      "    used. This is almost always (if not always) beneficial.\n",
      "ip : bool (default = False)\n",
      "    Set to ``True`` if the improved initial point suggestion due to [4]_\n",
      "    Section 4.3 is desired. Whether this is beneficial or not\n",
      "    depends on the problem.\n",
      "permc_spec : str (default = 'MMD_AT_PLUS_A')\n",
      "    (Has effect only with ``sparse = True``, ``lstsq = False``, ``sym_pos =\n",
      "    True``, and no SuiteSparse.)\n",
      "    A matrix is factorized in each iteration of the algorithm.\n",
      "    This option specifies how to permute the columns of the matrix for\n",
      "    sparsity preservation. Acceptable values are:\n",
      "\n",
      "    - ``NATURAL``: natural ordering.\n",
      "    - ``MMD_ATA``: minimum degree ordering on the structure of A^T A.\n",
      "    - ``MMD_AT_PLUS_A``: minimum degree ordering on the structure of A^T+A.\n",
      "    - ``COLAMD``: approximate minimum degree column ordering.\n",
      "\n",
      "    This option can impact the convergence of the\n",
      "    interior point algorithm; test different values to determine which\n",
      "    performs best for your problem. For more information, refer to\n",
      "    ``scipy.sparse.linalg.splu``.\n",
      "unknown_options : dict\n",
      "    Optional arguments not used by this particular solver. If\n",
      "    `unknown_options` is non-empty a warning is issued listing all\n",
      "    unused options.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "x : 1-D array\n",
      "    Solution vector.\n",
      "status : int\n",
      "    An integer representing the exit status of the optimization::\n",
      "\n",
      "     0 : Optimization terminated successfully\n",
      "     1 : Iteration limit reached\n",
      "     2 : Problem appears to be infeasible\n",
      "     3 : Problem appears to be unbounded\n",
      "     4 : Serious numerical difficulties encountered\n",
      "\n",
      "message : str\n",
      "    A string descriptor of the exit status of the optimization.\n",
      "iteration : int\n",
      "    The number of iterations taken to solve the problem.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This method implements the algorithm outlined in [4]_ with ideas from [8]_\n",
      "and a structure inspired by the simpler methods of [6]_.\n",
      "\n",
      "The primal-dual path following method begins with initial 'guesses' of\n",
      "the primal and dual variables of the standard form problem and iteratively\n",
      "attempts to solve the (nonlinear) Karush-Kuhn-Tucker conditions for the\n",
      "problem with a gradually reduced logarithmic barrier term added to the\n",
      "objective. This particular implementation uses a homogeneous self-dual\n",
      "formulation, which provides certificates of infeasibility or unboundedness\n",
      "where applicable.\n",
      "\n",
      "The default initial point for the primal and dual variables is that\n",
      "defined in [4]_ Section 4.4 Equation 8.22. Optionally (by setting initial\n",
      "point option ``ip=True``), an alternate (potentially improved) starting\n",
      "point can be calculated according to the additional recommendations of\n",
      "[4]_ Section 4.4.\n",
      "\n",
      "A search direction is calculated using the predictor-corrector method\n",
      "(single correction) proposed by Mehrota and detailed in [4]_ Section 4.1.\n",
      "(A potential improvement would be to implement the method of multiple\n",
      "corrections described in [4]_ Section 4.2.) In practice, this is\n",
      "accomplished by solving the normal equations, [4]_ Section 5.1 Equations\n",
      "8.31 and 8.32, derived from the Newton equations [4]_ Section 5 Equations\n",
      "8.25 (compare to [4]_ Section 4 Equations 8.6-8.8). The advantage of\n",
      "solving the normal equations rather than 8.25 directly is that the\n",
      "matrices involved are symmetric positive definite, so Cholesky\n",
      "decomposition can be used rather than the more expensive LU factorization.\n",
      "\n",
      "With default options, the solver used to perform the factorization depends\n",
      "on third-party software availability and the conditioning of the problem.\n",
      "\n",
      "For dense problems, solvers are tried in the following order:\n",
      "\n",
      "1. ``scipy.linalg.cho_factor``\n",
      "\n",
      "2. ``scipy.linalg.solve`` with option ``sym_pos=True``\n",
      "\n",
      "3. ``scipy.linalg.solve`` with option ``sym_pos=False``\n",
      "\n",
      "4. ``scipy.linalg.lstsq``\n",
      "\n",
      "For sparse problems:\n",
      "\n",
      "1. ``sksparse.cholmod.cholesky`` (if scikit-sparse and SuiteSparse are installed)\n",
      "\n",
      "2. ``scipy.sparse.linalg.factorized`` (if scikit-umfpack and SuiteSparse are installed)\n",
      "\n",
      "3. ``scipy.sparse.linalg.splu`` (which uses SuperLU distributed with SciPy)\n",
      "\n",
      "4. ``scipy.sparse.linalg.lsqr``\n",
      "\n",
      "If the solver fails for any reason, successively more robust (but slower)\n",
      "solvers are attempted in the order indicated. Attempting, failing, and\n",
      "re-starting factorization can be time consuming, so if the problem is\n",
      "numerically challenging, options can be set to  bypass solvers that are\n",
      "failing. Setting ``cholesky=False`` skips to solver 2,\n",
      "``sym_pos=False`` skips to solver 3, and ``lstsq=True`` skips\n",
      "to solver 4 for both sparse and dense problems.\n",
      "\n",
      "Potential improvements for combatting issues associated with dense\n",
      "columns in otherwise sparse problems are outlined in [4]_ Section 5.3 and\n",
      "[10]_ Section 4.1-4.2; the latter also discusses the alleviation of\n",
      "accuracy issues associated with the substitution approach to free\n",
      "variables.\n",
      "\n",
      "After calculating the search direction, the maximum possible step size\n",
      "that does not activate the non-negativity constraints is calculated, and\n",
      "the smaller of this step size and unity is applied (as in [4]_ Section\n",
      "4.1.) [4]_ Section 4.3 suggests improvements for choosing the step size.\n",
      "\n",
      "The new point is tested according to the termination conditions of [4]_\n",
      "Section 4.5. The same tolerance, which can be set using the ``tol`` option,\n",
      "is used for all checks. (A potential improvement would be to expose\n",
      "the different tolerances to be set independently.) If optimality,\n",
      "unboundedness, or infeasibility is detected, the solve procedure\n",
      "terminates; otherwise it repeats.\n",
      "\n",
      "The expected problem formulation differs between the top level ``linprog``\n",
      "module and the method specific solvers. The method specific solvers expect a\n",
      "problem in standard form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A @ x == b\n",
      "        x >= 0\n",
      "\n",
      "Whereas the top level ``linprog`` module expects a problem of form:\n",
      "\n",
      "Minimize::\n",
      "\n",
      "    c @ x\n",
      "\n",
      "Subject to::\n",
      "\n",
      "    A_ub @ x <= b_ub\n",
      "    A_eq @ x == b_eq\n",
      "     lb <= x <= ub\n",
      "\n",
      "where ``lb = 0`` and ``ub = None`` unless set in ``bounds``.\n",
      "\n",
      "The original problem contains equality, upper-bound and variable constraints\n",
      "whereas the method specific solver requires equality constraints and\n",
      "variable non-negativity.\n",
      "\n",
      "``linprog`` module converts the original problem to standard form by\n",
      "converting the simple bounds to upper bound constraints, introducing\n",
      "non-negative slack variables for inequality constraints, and expressing\n",
      "unbounded variables as the difference between two non-negative variables.\n",
      "\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "       optimizer for linear programming: an implementation of the\n",
      "       homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "       2000. 197-232.\n",
      ".. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "       Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "       March 2004. Available 2/25/2017 at\n",
      "       https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      ".. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "       programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      ".. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "       programming.\" Athena Scientific 1 (1997): 997.\n",
      ".. [10] Andersen, Erling D., et al. Implementation of interior point methods\n",
      "        for large scale linear programming. HEC/Universite de Geneve, 1996.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.optimize.show_options(method='bounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
